# Basic information
name = "fwd-sait-config00"
task = "sait-ddom" # the name should be listed in TASKNAME2TASK(util.py) 
mode = "train" # Option : [train, eval]

[architecture]
# [[1. Unet architecture]]
hidden_size = 1024
# depth = 4 
# Depth arg is not applied to Unet architecture.(bug). If want to change depth, refer to MLP class in forward.py
# activation function is fixed by Swish() but paper said they used ReLU(), which makes confusion

# [[2. Normalization]]
# Normalization strategy is suggested to show better results in training stage. (Design-bench, Arxiv 2021)
# In evaluation stage, args.condition is equal to task.y.max().
normalise_x = true
normalise_y = true

[training]
# [[1. Loss formulation]]
# uncomment argument if you want to score matching loss instead of sde-flow(Huang, NeurIPS 2021).
# score_matching = true

# [[2. Learning rate]]
learning_rate = 1e-3

# [[3. Dropout]]
# Dropout is not yet implemented
# dropout_p = 0.15

# [[4. Learning rate Finder]]
# auto_tune_lr = true # Find best learning rate proposal by Pytorch Lightning Trainer

# [[5. Learning rate scheduler]]
# To change learning rate schedule, please change configure_optimizers in forward.py
# Note that DSM, DiffusionScore class supports scheduling but sde-flow, DiffusionTest class does not support scheduling.
# So to do some more sophisticated learning schemes, implement scheduler there. 

# [[6. Epochs]]
# max_steps + checkpoint_every_n_steps can also be used
epochs = 1000
checkpoint_every_n_epochs = 100

# [[7. GPU-dependent setting]]
batch_size = 1024
num_workers = 4
use_gpu = true
which_gpu = 0

# [[7. Train/Val Split]]
val_frac = 0.05

[hyperparameter]
# [[1. Seed]]
seed = 777