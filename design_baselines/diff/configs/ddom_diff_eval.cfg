# Basic information
name = "sait-config00"
task = "sait-ddom" # the name should be listed in TASKNAME2TASK(util.py) 
mode = "eval" # Option : [train, eval]


[architecture]
# [[1. Unet architecture]]
hidden_size = 1024
# depth = 4 
# Depth arg is not applied to Unet architecture.(bug). If want to change depth, refer to MLP class in forward.py
# activation function is fixed by Swish() but paper said they used ReLU(), which makes confusion

# [[2. Normalization]]
# Normalization strategy is suggested to show better results in training stage. (Design-bench, Arxiv 2021)
# In evaluation stage, args.condition is equal to task.y.max().
normalise_x = true
normalise_y = true

# [[3. Random Variable for Hutchinson's Trace estimator ]]
# It is used to obtain a stochastic estimate of the trace of a matrix.
# Refer to https://blog.shakirm.com/2015/09/machine-learning-trick-of-the-day-3-hutchinsons-trick/
# rademacher : 1 or -1(discrete value)
# gaussian : Normal distribution(continuous value within -1 ~ 1)
vtype = 'rademacher'

[training]
# [[1. Loss formulation]]
# uncomment argument if you want to score matching loss instead of sde-flow(Huang, NeurIPS 2021).
# score_matching = true

# [[2. Learning rate]]
learning_rate = 1e-3
dropout_p = 0.15

# [[3. Learning rate Finder]]
# auto_tune_lr = true # Find best learning rate proposal by Pytorch Lightning Trainer

# [[4. Learning rate scheduler]]
# To change learning rate schedule, please change configure_optimizers in nets.py.
# Note that DSM, DiffusionScore class supports scheduling but sde-flow, DiffusionTest class does not support scheduling.
# So to do some more sophisticated learning schemes, implement scheduler there. 

# [[5. Epochs]]
# max_steps + checkpoint_every_n_steps can also be used
epochs = 1000
checkpoint_every_n_epochs = 100

# [[6. GPU-dependent setting]]
batch_size = 1024
num_workers = 4
use_gpu = true
which_gpu = 0

# [[7. Train/Val Split]]
val_frac = 0.05

[sampler]
# Using non-uniform sampling to debias
# https://github.com/CW-Huang/sdeflow-light?tab=readme-ov-file#how-it-works
debias = true

[hyperparameter]
# [[1. Seed]]
seed = 777

# [[2. Condition for conditioinal diffusion model]]
# usually set to the maxima from full dataset. Note that y should be normalized if you set the condition as 1..
condition = 1.

# [[3. Variance schedule for forward process]]
beta_min = 0.01
beta_max = 2.0

# [[4. Classifier-free guidance]]
# Seemingly unimplemented in the code.
gamma = 5.0 

# [[5. Reweighting term]]
# It is set 90 as default. Option : [90,70,50] / 'reweighting temp' described in adaptive_temp_v2(util.py).
temp = 90 